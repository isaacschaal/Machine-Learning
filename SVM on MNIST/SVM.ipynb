{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from shutil import copyfileobj\n",
    "from six.moves import urllib\n",
    "from sklearn.datasets.base import get_data_home\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing The Data\n",
    "\n",
    "The below code for importing the entire MNIST dataset was provided by the instructor in a class slack channel.\n",
    "The fetch_mldata function is depreceated, but still works for now.\n",
    "\n",
    "The MNIST data set is a set of integers drawn by humans. Each data point has the drawing and the correct integer label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def fetch_mnist(data_home=None):\n",
    "    mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    data_home = os.path.join(data_home, 'mldata')\n",
    "    if not os.path.exists(data_home):\n",
    "        os.makedirs(data_home)\n",
    "    mnist_save_path = os.path.join(data_home, \"mnist-original.mat\")\n",
    "    if not os.path.exists(mnist_save_path):\n",
    "        mnist_url = urllib.request.urlopen(mnist_alternative_url)\n",
    "        with open(mnist_save_path, \"wb\") as matlab_file:\n",
    "            copyfileobj(mnist_url, matlab_file)\n",
    "\n",
    "fetch_mnist()\n",
    "mnist = fetch_mldata(\"MNIST original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the Data\n",
    "\n",
    "To prepare the data set, I selected only the 3s and 7s, and converted the data into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = []\n",
    "target = []\n",
    "\n",
    "for i in range(len(mnist['target'])):\n",
    "    if mnist['target'][i] == 3 or mnist['target'][i] == 7:\n",
    "        image.append(mnist['data'][i])\n",
    "        target.append(mnist['target'][i])\n",
    "        \n",
    "image_array = np.array(image)\n",
    "target_array = np.array(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data point is an array of 738 pixels, with a greyscale value (indicating how black the pixel is). In order to visualize them, we must reshape the data into a 28 x 28 array. The 738 x 1 array is the correct format for using our classifier. The visualizations below show two drawings each for 3 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape the data for showing the integer drawing with matplotlib\n",
    "display =[image_array[i].reshape(28,28) for i in [0,1,-1,-2]]\n",
    "\n",
    "plt.figure(figsize = (10,6) )\n",
    "for index, image in enumerate(display):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "I will be using a Support Vector Machine (SVM) to classify. SVMs offer several benefits for this situation.  Firstly, they are effective in high dimensional spaces. Image classification is high dimensional as we are looking at each indiviudal pixel. Secondly, we have the ability to choose different kernel functions. These kernel functions bring our data into even higher dimensions in order to find better classifiers. We will try three kernel functions (linear, polynomial, and radial basis function) to find which one performs best.\n",
    "\n",
    "\n",
    "### Tuning Hyperparameters\n",
    "\n",
    "We will tune the hyperparamters of our SVM using cross validation. We create a list of possible parameters and use Grid Search to find the optimal params. We are optimzing over several parameters. The first is C, the regularization parameter. This value acts as a penalty for misclassifcation, with a low c giving a low penalty and a high c giving a high penalty. This is applicable to all 3 kernels. The second parameter we are optimizing is just for the polynomial kernel and is the degree. This changes the number and type of additonal dimensions the kernel has. The third parameter is gamma, which is just for the rbf kernel. The gamma changes the variance of the rbf kernels, with a larger gamma, the radial basis functions narrow and can adapt more to the data. However, if it is too high it can lead to overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "X_train,X_test,y_train,y_test = train_test_split(image_array,target_array,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of 105 | elapsed:  7.3min remaining:   56.8s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9958429561200923\n",
      "{'C': 10, 'gamma': 1e-07, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters to be optimized\n",
    "param_grids = [\n",
    "    {'kernel': ['linear'], 'C':[0.01, 0.1, 1, 10, 100]},\n",
    "    {'kernel': ['poly'], 'C':[0.01, 0.1, 1, 10, 100], 'degree': [2,3,4]},\n",
    "    {'kernel': ['rbf'], 'C':[0.01, 0.1, 1, 10, 100], 'gamma':[1e-8,1e-7,1e-6]}\n",
    "    ]\n",
    "\n",
    "# Run Grid Search\n",
    "search = GridSearchCV(svm.SVC(), param_grids, cv=3, verbose = 10, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing kernel was the rbf kernel with C = 10 and gamma = 1e-7 . However, we would like to see what the best paramaters for the other kernels were as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.01, 'kernel': 'linear'}</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.981801</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.1, 'kernel': 'linear'}</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.981801</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1, 'kernel': 'linear'}</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.981801</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 10, 'kernel': 'linear'}</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.981801</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 100, 'kernel': 'linear'}</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.981801</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            params param_kernel  mean_test_score  \\\n",
       "0  {'C': 0.01, 'kernel': 'linear'}       linear         0.981801   \n",
       "1   {'C': 0.1, 'kernel': 'linear'}       linear         0.981801   \n",
       "2     {'C': 1, 'kernel': 'linear'}       linear         0.981801   \n",
       "3    {'C': 10, 'kernel': 'linear'}       linear         0.981801   \n",
       "4   {'C': 100, 'kernel': 'linear'}       linear         0.981801   \n",
       "\n",
       "   rank_test_score  \n",
       "0               26  \n",
       "1               26  \n",
       "2               26  \n",
       "3               26  \n",
       "4               26  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(search.cv_results_)\n",
    "df = df[['params','param_kernel','mean_test_score','rank_test_score']]\n",
    "df.loc[df['param_kernel'] == 'linear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that changing the C value had no impact on our linear kernel. Thus, choosing a C of 1 will give us the optimal SVM with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 0.01, 'degree': 2, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.995566</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'C': 0.01, 'degree': 3, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994734</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'C': 0.01, 'degree': 4, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994088</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'C': 0.1, 'degree': 2, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.995566</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'C': 0.1, 'degree': 3, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994734</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'C': 0.1, 'degree': 4, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994088</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'C': 1, 'degree': 2, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.995566</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'C': 1, 'degree': 3, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994734</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'C': 1, 'degree': 4, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994088</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'C': 10, 'degree': 2, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.995566</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'C': 10, 'degree': 3, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994734</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'C': 10, 'degree': 4, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994088</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'C': 100, 'degree': 2, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.995566</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'C': 100, 'degree': 3, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994734</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'C': 100, 'degree': 4, 'kernel': 'poly'}</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.994088</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        params param_kernel  mean_test_score  \\\n",
       "5   {'C': 0.01, 'degree': 2, 'kernel': 'poly'}         poly         0.995566   \n",
       "6   {'C': 0.01, 'degree': 3, 'kernel': 'poly'}         poly         0.994734   \n",
       "7   {'C': 0.01, 'degree': 4, 'kernel': 'poly'}         poly         0.994088   \n",
       "8    {'C': 0.1, 'degree': 2, 'kernel': 'poly'}         poly         0.995566   \n",
       "9    {'C': 0.1, 'degree': 3, 'kernel': 'poly'}         poly         0.994734   \n",
       "10   {'C': 0.1, 'degree': 4, 'kernel': 'poly'}         poly         0.994088   \n",
       "11     {'C': 1, 'degree': 2, 'kernel': 'poly'}         poly         0.995566   \n",
       "12     {'C': 1, 'degree': 3, 'kernel': 'poly'}         poly         0.994734   \n",
       "13     {'C': 1, 'degree': 4, 'kernel': 'poly'}         poly         0.994088   \n",
       "14    {'C': 10, 'degree': 2, 'kernel': 'poly'}         poly         0.995566   \n",
       "15    {'C': 10, 'degree': 3, 'kernel': 'poly'}         poly         0.994734   \n",
       "16    {'C': 10, 'degree': 4, 'kernel': 'poly'}         poly         0.994088   \n",
       "17   {'C': 100, 'degree': 2, 'kernel': 'poly'}         poly         0.995566   \n",
       "18   {'C': 100, 'degree': 3, 'kernel': 'poly'}         poly         0.994734   \n",
       "19   {'C': 100, 'degree': 4, 'kernel': 'poly'}         poly         0.994088   \n",
       "\n",
       "    rank_test_score  \n",
       "5                 3  \n",
       "6                 8  \n",
       "7                16  \n",
       "8                 3  \n",
       "9                 8  \n",
       "10               16  \n",
       "11                3  \n",
       "12                8  \n",
       "13               16  \n",
       "14                3  \n",
       "15                8  \n",
       "16               16  \n",
       "17                3  \n",
       "18                8  \n",
       "19               16  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['param_kernel'] == 'poly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that C had no impact on the polynomial kernel, but the degree did. The optimal poylnomial kernel has C=1, and degree = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Results\n",
    "\n",
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 6.37 seconds to train.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         3.0       0.98      0.98      0.98      1816\n",
      "         7.0       0.98      0.97      0.98      1793\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      3609\n",
      "   macro avg       0.98      0.98      0.98      3609\n",
      "weighted avg       0.98      0.98      0.98      3609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel = 'linear', C=1)\n",
    "start = time.time()\n",
    "clf.fit(X_train,y_train )\n",
    "end = time.time()\n",
    "print ('It took {0} seconds to train.'.format(round(end - start,2)))\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = (clf.predict(X_test))\n",
    "\n",
    "# Show the results\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "\n",
    "(The warning explained that the default setting of gamma (different than the gamma of the RBF) will change in the future, but we are fine for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 4.68 seconds to train.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         3.0       1.00      1.00      1.00      1816\n",
      "         7.0       1.00      1.00      1.00      1793\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3609\n",
      "   macro avg       1.00      1.00      1.00      3609\n",
      "weighted avg       1.00      1.00      1.00      3609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel = 'poly', C=0.01, degree =3)\n",
    "start = time.time()\n",
    "clf.fit(X_train,y_train )\n",
    "end = time.time()\n",
    "print ('It took {0} seconds to train.'.format(round(end - start,2)))\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = (clf.predict(X_test))\n",
    "\n",
    "# Show the results\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 4.87 seconds to train.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         3.0       1.00      0.99      1.00      1816\n",
      "         7.0       0.99      1.00      1.00      1793\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      3609\n",
      "   macro avg       1.00      1.00      1.00      3609\n",
      "weighted avg       1.00      1.00      1.00      3609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel = 'rbf', C=10, gamma = 1e-07)\n",
    "start = time.time()\n",
    "clf.fit(X_train,y_train )\n",
    "end = time.time()\n",
    "print ('It took {0} seconds to train.'.format(round(end - start,2)))\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = (clf.predict(X_test))\n",
    "\n",
    "# Show the results\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Of the three kernels, the linear kernel took the longest to train at 6.37 seconds, with the polynomial time of 4.68 seconds beating the RBF time of 4.87. The linear kernel got a .98 accuracy overall, performing the worst. The Grid Search predicted that the RBF would perform the best, but in fact the polynomial kernel did slightly better. The weighted avg for both was 1.0, but the polynomial kernel actually got all of the correct, while the RBF kernel misclassifed at least one 3 as a 7. The results for the two were very close, but due to the slight accuracy increase and less training time, the polynomial kernel was the best for classfication of 3 and 7 MNIST digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifcation on 3 classes\n",
    "\n",
    "To extend this analysis, I performed classification with 3 classes. I got all data points for 3,7 and 2, and did the same cross validation optimization. To reduce the length of the cross validation, I took only half of the data set (ranomdally selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of 105 | elapsed:  7.4min remaining:   57.2s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:  8.7min finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9830193356470005\n",
      "{'C': 0.01, 'degree': 3, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "image = []\n",
    "target = []\n",
    "\n",
    "for i in range(len(mnist['target'])):\n",
    "    if mnist['target'][i] == 3 or mnist['target'][i] == 7 or mnist['target'][i] == 2:\n",
    "        if random.random() < 0.5:\n",
    "            image.append(mnist['data'][i])\n",
    "            target.append(mnist['target'][i])\n",
    "\n",
    "        \n",
    "image_array = np.array(image)\n",
    "target_array = np.array(target)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(image_array,target_array,test_size=0.25)\n",
    "\n",
    "param_grids = [\n",
    "    {'kernel': ['linear'], 'C':[0.01, 0.1, 1, 10, 100]},\n",
    "    {'kernel': ['poly'], 'C':[0.01, 0.1, 1, 10, 100], 'degree': [2,3,4]},\n",
    "    {'kernel': ['rbf'], 'C':[0.01, 0.1, 1, 10, 100], 'gamma':[1e-8,1e-7,1e-6]}\n",
    "    ]\n",
    "\n",
    "search = GridSearchCV(svm.SVC(), param_grids, cv=3, verbose = 10, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "With 3 classes, the polynomial kernel with degree 3 performed the best, beating all potential rbf kernels. The model took slightly longer to train than the models with only 2 classes. The model performed at the level predicted by grid search, with a 0.98 avg overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 7.05 seconds to train.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0       0.98      0.99      0.98       897\n",
      "         3.0       0.99      0.98      0.98       886\n",
      "         7.0       0.98      0.99      0.99       907\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2690\n",
      "   macro avg       0.98      0.98      0.98      2690\n",
      "weighted avg       0.98      0.98      0.98      2690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel = 'poly', C=0.01, degree =3)\n",
    "start = time.time()\n",
    "clf.fit(X_train,y_train )\n",
    "end = time.time()\n",
    "print ('It took {0} seconds to train.'.format(round(end - start,2)))\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = (clf.predict(X_test))\n",
    "\n",
    "# Show the results\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
